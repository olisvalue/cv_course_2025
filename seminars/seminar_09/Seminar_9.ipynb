{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семинар 9 - Методы построения оптического потока по последовательности изображений\n",
    "\n",
    "**Этот семинар содержит оцениваемое домашнее задание**\n",
    "\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Источник - https://habr.com/ru/post/201406/\n",
    "\n",
    "$\\textbf{Task statement}$: Оптический поток (ОП) – изображение видимого движения, представляющее собой сдвиг каждой точки (пикселя) между двумя изображениями.\n",
    "\n",
    "По сути, он представляет собой поле скоростей. Суть ОП в том, что для каждой точки изображения $I_{t_0} (\\vec{r})$ находится такой вектор сдвига $\\delta \\vec{r}$, чтобы было соответсвие между исходной точкой и точкой на следущем фрейме $I_{t_1} (\\vec{r} + \\delta \\vec{r})$. В качестве метрики соответвия берут близость интенсивности пикселей, беря во внимание маленькую разницу по времени между кадрами: $\\delta{t} = t_{1} - t_{0}$. В более точных методах точку можно привязывать к объекту на основе, например, выделения ключевых точек, а также считать градиенты вокруг точки, лапласианы и проч.\n",
    "\n",
    "$\\textbf{For what}$: Определение собственной скорости, Определение локализации, Улучшение методов трекинга объектов, сегментации, Детектирование событий, Сжатие видеопотока и проч.\n",
    "\n",
    "![](data/tennis.png)\n",
    "\n",
    "Разделяют 2 вида оптического потока - плотный (dense) [Farneback method, neural nets], работающий с целым изображением, и выборочный (sparse) [Lucas-Kanade method], работающий с ключевыми точками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !wget https://www.bogotobogo.com/python/OpenCV_Python/images/mean_shift_tracking/slow_traffic_small.mp4 -O data/slow_traffic_small.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lucas-Kanade (sparse)\n",
    "\n",
    "Пусть $I_{1} = I(x, y, t_{1})$ интенсивность в некоторой точке (x, y) на первом изображении (т. е. в момент времени t). На втором изображении эта точка сдвинулась на (dx, dy), при этом прошло время dt, тогда $I_{2} = I(x + dx, y + dx, t_{1} + dt) \\approx I_{1} + I_{x}dx + I_{y}dy +  I_{t}dt$. Из постановки задачи следует, что интенсивность пикселя не изменилась, тогда $I_{1} = I_{2}$. Далее определяем $dx, dy$.\n",
    "\n",
    "Самое простое решение проблемы – алгоритм Лукаса-Канаде. У нас же на изображении объекты размером больше 1 пикселя, значит, скорее всего, в окрестности текущей точки у других точек будут примерно такие же сдвиги. Поэтому мы возьмем окно вокруг этой точки и минимизируем (по МНК) в нем суммарную погрешность с весовыми коэффициентами, распределенными по Гауссу, то есть так, чтобы наибольший вес имели пиксели, ближе всего находящиеся к исследуемому.\n",
    "\n",
    "**Полезные материалы:** \n",
    "- цикл видео-лекций от First Principles of Computer Vision, посвященный Optical Flow и алгоритму Lucas-Kanade: https://youtube.com/playlist?list=PL2zRqk16wsdoYzrWStffqBAoUY8XdvatV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вопрос 1\n",
    "\n",
    "Перечислите три основных предположения, на которых базируется метод Lucas-Kanade. Почему каждое из них важно для корректной работы алгоритма?\n",
    "\n",
    "**Ответ:**\n",
    "\n",
    "Яркостная постоянство: интенсивность каждого пикселя остаётся неизменной при смещении между кадрами, то есть I(x,y,t)=I(x+u,y+v,t+1). Это важно, чтобы линейная аппроксимация оставалась корректной и позволяла связать пространственные и временные градиенты яркости \n",
    "\n",
    "Малые перемещения: смещения u,v между соседними кадрами должны быть малы (обычно < 1 пикселя), чтобы первые члены разложения Тейлора давали достаточно точную аппроксимацию нелинейного сдвига \n",
    "\n",
    "Пространственная когерентность (гладкость потока): в небольшом окне вокруг точки поток считается постоянным, что даёт дополнительные уравнения для надёжного решения СЛАУ и уменьшает чувствительность к шуму\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вопрос 2\n",
    "\n",
    "Объясните, зачем нужен пирамидальный подход в алгоритме Lucas-Kanade. Какую проблему он решает и как именно?\n",
    "\n",
    "**Ответ:**\n",
    "При больших смещениях между кадрами разложение Тейлора теряет точность, так как нарушается предположение малых перемещений. Пирамидальная схема (снижение разрешения на верхнем уровне и постепенный переход к исходному) преобразует крупные смещения в малые, где алгоритм Lucas–Kanade быстро оценивает грубый поток"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вопрос 3\n",
    "\n",
    "С какими проблемами может столкнуться алгоритм Lucas-Kanade при отслеживании точек на видео? Назовите минимум три ограничения.\n",
    "\n",
    "**Ответ:**\n",
    "\n",
    "Окклюзии: скрытие или появление объектов приводит к невозможности соотнести пиксели между кадрами и вызывает ошибки в оценке потока \n",
    "\n",
    "Изменения освещённости: нарушение яркостного постоянства (например, при мерцании или затенении) искажает связь яркости между кадрами \n",
    "\n",
    "Текстурно-плохие области: в однотонных или слабоструктурированных зонах нет устойчивых градиентов, что делает матрицу нормальных уравнений плохо обусловленной"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1\n",
    "\n",
    "Напишите реализацию Лукаса-Канаде c помощью numpy и cv2. Сравните с реализацией `cv2.calcOpticalFlowPyrLK`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def build_image_pyramid(image, num_levels, scale_factor=0.5):\n",
    "    pyramid = [image.copy()]\n",
    "    for i in range(1, num_levels):\n",
    "        prev = pyramid[-1]\n",
    "        h, w = prev.shape[:2]\n",
    "        new_size = (max(1, int(w * scale_factor)), max(1, int(h * scale_factor)))\n",
    "        resized = cv2.resize(prev, new_size, interpolation=cv2.INTER_LINEAR)\n",
    "        pyramid.append(resized)\n",
    "    return pyramid\n",
    "\n",
    "\n",
    "def compute_image_gradients(image):\n",
    "    Ix = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=3)\n",
    "    Iy = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=3)\n",
    "    return Ix, Iy\n",
    "\n",
    "\n",
    "def compute_lk_optical_flow_point(Ix, Iy, It, window_size=5, eig_threshold=1e-4):\n",
    "    half = window_size // 2\n",
    "    Ix_w = Ix.flatten()\n",
    "    Iy_w = Iy.flatten()\n",
    "    It_w = It.flatten()\n",
    "    A11 = np.sum(Ix_w * Ix_w)\n",
    "    A12 = np.sum(Ix_w * Iy_w)\n",
    "    A22 = np.sum(Iy_w * Iy_w)\n",
    "    G = np.array([[A11, A12], [A12, A22]])\n",
    "    # Проверка обусловленности через собственные значения\n",
    "    eigs = np.linalg.eigvals(G)\n",
    "    if np.min(eigs) < eig_threshold:\n",
    "        return None, None\n",
    "    b = -np.array([np.sum(Ix_w * It_w), np.sum(Iy_w * It_w)])\n",
    "    try:\n",
    "        nu = np.linalg.inv(G) @ b\n",
    "        return nu[0], nu[1]\n",
    "    except np.linalg.LinAlgError:\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def compute_lk_optical_flow_for_patch(prev_patch, curr_patch, window_size=5):\n",
    "    Ix, Iy = compute_image_gradients(prev_patch)\n",
    "    It = (curr_patch - prev_patch).astype(np.float64)\n",
    "    return compute_lk_optical_flow_point(Ix, Iy, It, window_size)\n",
    "\n",
    "\n",
    "def track_point_with_pyramid_lk(prev_pyr, curr_pyr, point,\n",
    "                               window_size=15, max_iterations=10, epsilon=0.01):\n",
    "    levels = len(prev_pyr)\n",
    "    # Изначальная точка на уровне 0\n",
    "    u, v = 0.0, 0.0\n",
    "    # Координаты в текущих уровнях\n",
    "    x, y = point\n",
    "    # Начинаем с верхнего (коэффициент scale_factor**(levels-1))\n",
    "    for lvl in reversed(range(levels)):\n",
    "        scale = 1.0 / (2 ** lvl)\n",
    "        x_lvl = x * scale + u\n",
    "        y_lvl = y * scale + v\n",
    "        prev = prev_pyr[lvl]\n",
    "        curr = curr_pyr[lvl]\n",
    "        h, w = prev.shape\n",
    "        # Итеративное уточнение\n",
    "        for _ in range(max_iterations):\n",
    "            x0, y0 = int(round(x_lvl)), int(round(y_lvl))\n",
    "            half = window_size // 2\n",
    "            # Проверка границ\n",
    "            if x0 - half < 0 or x0 + half >= w or y0 - half < 0 or y0 + half >= h:\n",
    "                return None\n",
    "            prev_patch = prev[y0-half:y0+half+1, x0-half:x0+half+1]\n",
    "            curr_patch = curr[y0-half:y0+half+1, x0-half:x0+half+1]\n",
    "            du, dv = compute_lk_optical_flow_for_patch(prev_patch, curr_patch, window_size)\n",
    "            if du is None:\n",
    "                return None\n",
    "            x_lvl += du\n",
    "            y_lvl += dv\n",
    "            if np.hypot(du, dv) < epsilon:\n",
    "                break\n",
    "        # Переносим смещение на следующий (более высокое разрешение)\n",
    "        u = x_lvl - x * scale\n",
    "        v = y_lvl - y * scale\n",
    "        # Увеличиваем базовую точку для следующего уровня\n",
    "        u *= 2\n",
    "        v *= 2\n",
    "    # Финальная позиция на уровне 0\n",
    "    new_x = x + u / 2\n",
    "    new_y = y + v / 2\n",
    "    return (new_x, new_y)\n",
    "\n",
    "\n",
    "def lucas_kanade_optical_flow(prev_frame, curr_frame, points,\n",
    "                              window_size=15, num_pyramid_levels=3,\n",
    "                              max_iterations=10, epsilon=0.01):\n",
    "    # Приводим к grayscale и нормализуем\n",
    "    if prev_frame.ndim == 3:\n",
    "        prev = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY).astype(np.float64) / 255.0\n",
    "        curr = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY).astype(np.float64) / 255.0\n",
    "    else:\n",
    "        prev = prev_frame.astype(np.float64) / 255.0\n",
    "        curr = curr_frame.astype(np.float64) / 255.0\n",
    "\n",
    "    prev_pyr = build_image_pyramid(prev, num_pyramid_levels)\n",
    "    curr_pyr = build_image_pyramid(curr, num_pyramid_levels)\n",
    "\n",
    "    new_points = []\n",
    "    status = []\n",
    "    for pt in points:\n",
    "        res = track_point_with_pyramid_lk(\n",
    "            prev_pyr, curr_pyr, pt, window_size, max_iterations, epsilon\n",
    "        )\n",
    "        if res is None:\n",
    "            new_points.append(pt)\n",
    "            status.append(0)\n",
    "        else:\n",
    "            new_points.append(res)\n",
    "            status.append(1)\n",
    "    return np.array(new_points, dtype=np.float32), np.array(status, dtype=np.uint8)\n",
    "\n",
    "\n",
    "def demo_optical_flow(video_path='data/slow_traffic_small.mp4', output_path='output_my_LK.mp4'):\n",
    "    \"\"\"\n",
    "    Демонстрация работы алгоритма на видео.\n",
    "\n",
    "    Args:\n",
    "        video_path: Путь к входному видео\n",
    "        output_path: Путь для сохранения результата\n",
    "    \"\"\"\n",
    "    # Открываем видео\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Получаем параметры видео\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Настраиваем запись выходного видео\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    # Параметры для обнаружения углов Shi-Tomasi\n",
    "    feature_params = dict(\n",
    "        maxCorners=100,\n",
    "        qualityLevel=0.3,\n",
    "        minDistance=7,\n",
    "        blockSize=7\n",
    "    )\n",
    "\n",
    "    # Берем первый кадр и находим в нем углы\n",
    "    ret, old_frame = cap.read()\n",
    "    old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)\n",
    "    p0 = cv2.goodFeaturesToTrack(old_gray, mask=None, **feature_params)\n",
    "    p0 = p0.reshape(-1, 2)  # Преобразуем в формат [[x1, y1], [x2, y2], ...]\n",
    "\n",
    "    # Сохраняем изначальные точки для отслеживания через все видео\n",
    "    initial_points = p0.copy()\n",
    "\n",
    "    # Создаем маску для рисования\n",
    "    mask = np.zeros_like(old_frame)\n",
    "\n",
    "    # Создаем случайные цвета для визуализации\n",
    "    color = np.random.randint(0, 255, (len(p0), 3))\n",
    "\n",
    "    from tqdm import tqdm\n",
    "    for i in tqdm(range(length - 1)):  # -1 потому что первый кадр мы уже прочитали\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            print('No frames grabbed!')\n",
    "            break\n",
    "\n",
    "        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Вычисляем оптический поток с помощью нашей реализации\n",
    "        p1, st = lucas_kanade_optical_flow(\n",
    "            old_gray,\n",
    "            frame_gray,\n",
    "            p0,\n",
    "            window_size=15,\n",
    "            num_pyramid_levels=3\n",
    "        )\n",
    "\n",
    "        # Выбираем хорошие точки\n",
    "        good_new = p1[st == 1]\n",
    "        good_old = p0[st == 1]\n",
    "\n",
    "        # Рисуем треки\n",
    "        for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
    "            a, b = new\n",
    "            c, d = old\n",
    "            mask = cv2.line(mask, (int(a), int(b)), (int(c), int(d)), color[i % len(color)].tolist(), 2)\n",
    "            frame = cv2.circle(frame, (int(a), int(b)), 5, color[i % len(color)].tolist(), -1)\n",
    "\n",
    "        # Объединяем кадр и маску\n",
    "        img = cv2.add(frame, mask)\n",
    "\n",
    "        # Записываем результат\n",
    "        out.write(img)\n",
    "\n",
    "        # Обновляем предыдущий кадр\n",
    "        old_gray = frame_gray.copy()\n",
    "\n",
    "        # Обновляем точки, но только те, которые успешно отслежены\n",
    "        p0[st == 1] = good_new\n",
    "\n",
    "    # Освобождаем ресурсы\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "    print(f\"Результат сохранен в {output_path}\")\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x5634504d/'MP4V' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "  0%|          | 0/913 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 913/913 [00:58<00:00, 15.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результат сохранен в output_my_LK.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result_path = demo_optical_flow(video_path='data/slow_traffic_small.mp4', output_path='output_my_LK.mp4')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Релизация OpenCV - cv2.calcOpticalFlowPyrLK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_optical_flow_opencv(video_path='data/slow_traffic_small.mp4', output_path='output_my_LK.mp4'):\n",
    "    \"\"\"\n",
    "    Демонстрация работы алгоритма на видео с использованием cv2.calcOpticalFlowPyrLK.\n",
    "\n",
    "    Args:\n",
    "        video_path: Путь к входному видео\n",
    "        output_path: Путь для сохранения результата\n",
    "    \"\"\"\n",
    "    # Открываем видео\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Получаем параметры видео\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Настраиваем запись выходного видео\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    # Параметры для обнаружения углов Shi-Tomasi\n",
    "    feature_params = dict(\n",
    "        maxCorners=100,\n",
    "        qualityLevel=0.3,\n",
    "        minDistance=7,\n",
    "        blockSize=7\n",
    "    )\n",
    "\n",
    "    # Параметры для Lucas-Kanade оптического потока\n",
    "    lk_params = dict(\n",
    "        winSize=(15, 15),\n",
    "        maxLevel=3,\n",
    "        criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03)\n",
    "    )\n",
    "\n",
    "    # Берем первый кадр и находим в нем углы\n",
    "    ret, old_frame = cap.read()\n",
    "    old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)\n",
    "    p0 = cv2.goodFeaturesToTrack(old_gray, mask=None, **feature_params)\n",
    "\n",
    "    # Создаем маску для рисования\n",
    "    mask = np.zeros_like(old_frame)\n",
    "\n",
    "    # Создаем случайные цвета для визуализации\n",
    "    color = np.random.randint(0, 255, (100, 3))\n",
    "\n",
    "    from tqdm import tqdm\n",
    "    for i in tqdm(range(length - 1)):  # -1 потому что первый кадр мы уже прочитали\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            print('No frames grabbed!')\n",
    "            break\n",
    "\n",
    "        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Вычисляем оптический поток с помощью встроенной функции cv2.calcOpticalFlowPyrLK\n",
    "        p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params)\n",
    "\n",
    "        # Выбираем хорошие точки\n",
    "        if p1 is not None:\n",
    "            good_new = p1[st == 1]\n",
    "            good_old = p0[st == 1]\n",
    "\n",
    "        # Рисуем треки\n",
    "        for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
    "            a, b = new.ravel()\n",
    "            c, d = old.ravel()\n",
    "            mask = cv2.line(mask, (int(a), int(b)), (int(c), int(d)), color[i % len(color)].tolist(), 2)\n",
    "            frame = cv2.circle(frame, (int(a), int(b)), 5, color[i % len(color)].tolist(), -1)\n",
    "\n",
    "        # Объединяем кадр и маску\n",
    "        img = cv2.add(frame, mask)\n",
    "\n",
    "        # Записываем результат\n",
    "        out.write(img)\n",
    "\n",
    "        # Обновляем предыдущий кадр\n",
    "        old_gray = frame_gray.copy()\n",
    "\n",
    "        # Обновляем точки, но только те, которые успешно отслежены\n",
    "        p0 = good_new.reshape(-1, 1, 2)\n",
    "\n",
    "    # Освобождаем ресурсы\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "    print(f\"Результат сохранен в {output_path}\")\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x5634504d/'MP4V' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 913/913 [00:08<00:00, 105.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результат сохранен в output_opencv_LK.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result_path = demo_optical_flow_opencv(video_path='data/slow_traffic_small.mp4', output_path='output_opencv_LK.mp4')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2\n",
    "\n",
    "В базовой реализации у кода есть одна важная проблема - ключевые точки инициализируются единожды. В реальных задачах необходимо отслеживать точки, которые исчезают из кадра и появляются в других местах. Реализуйте механизм, который будет отслеживать точки, которые пропадают из кадра и добавлять новые точки в те места, где они появляются. Для этого вам нужно будет реализовать механизм поиска новых точек на изображении."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x5634504d/'MP4V' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "  1%|          | 9/914 [00:00<00:10, 85.26it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 913/914 [00:10<00:00, 86.23it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No frames grabbed!\n"
     ]
    }
   ],
   "source": [
    "video_path='data/slow_traffic_small.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps    = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "out = cv2.VideoWriter('output_fixed_LK.mp4', fourcc, fps, (width, height))\n",
    "\n",
    "feature_params = dict(\n",
    "    maxCorners = 100,\n",
    "    qualityLevel = 0.3,\n",
    "    minDistance = 7,\n",
    "    blockSize = 7,\n",
    ")\n",
    "lk_params = dict(\n",
    "    winSize  = (15, 15),\n",
    "    maxLevel = 2,\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03),\n",
    ")\n",
    "color = np.random.randint(0, 255, (100, 3))\n",
    "ret, old_frame = cap.read()\n",
    "old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)\n",
    "p0 = cv2.goodFeaturesToTrack(old_gray, mask = None, **feature_params)\n",
    "mask = np.zeros_like(old_frame)\n",
    "\n",
    "for i in tqdm(range(length)):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print('No frames grabbed!')\n",
    "        break\n",
    "\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    p1, st, _ = cv2.calcOpticalFlowPyrLK(\n",
    "        prevImg=old_gray,\n",
    "        nextImg=frame_gray,\n",
    "        prevPts=p0,\n",
    "        nextPts=None,\n",
    "        **lk_params,\n",
    "    )\n",
    "\n",
    "    if p1 is not None:\n",
    "        good_new = p1[st==1]\n",
    "        good_old = p0[st==1]\n",
    "\n",
    "    for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
    "        a, b = new.ravel()\n",
    "        c, d = old.ravel()\n",
    "        mask = cv2.line(mask, (int(a), int(b)), (int(c), int(d)), color[i].tolist(), 2)\n",
    "        frame = cv2.circle(frame, (int(a), int(b)), 5, color[i].tolist(), -1)\n",
    "    img = cv2.add(frame, mask)\n",
    "\n",
    "    if p1 is not None and len(good_new) > 0:\n",
    "        old_gray = frame_gray.copy()\n",
    "        p0 = good_new.reshape(-1, 1, 2)\n",
    "\n",
    "    # re-detect features periodically\n",
    "    if len(p0) < feature_params['maxCorners'] // 2 and \\\n",
    "        any([(p > old_frame.shape).all() for p in p0.ravel()]):\n",
    "        p0 = cv2.goodFeaturesToTrack(old_gray, mask=None, **feature_params)\n",
    "\n",
    "    out.write(img)\n",
    "\n",
    "cap.release()\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вопрос 4\n",
    "\n",
    "В чем основное отличие разреженного (sparse) оптического потока Lucas-Kanade от плотного (dense) оптического потока (например, метода Farneback)?\n",
    "\n",
    "**Ответ:**\n",
    "Lucas–Kanade (sparse): вычисляет поток только в заранее выбранных точках-характеристиках (углы Shi-Tomasi, FAST и т.д.).\n",
    "\n",
    "Farneback (dense): аппроксимирует поле движения для каждого пикселя кадра, давая непрерывную карту смещений."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Farneback (dense)\n",
    "\n",
    "Метод Farneback носит несколько более глобальный характер, чем метод Лукаса-Канаде. Он опирается на предположение о том, что на всем изображении оптический поток будет достаточно гладким."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вопрос 5\n",
    "\n",
    "Перечислите основные шаги алгоритма Farneback для расчета оптического потока.\n",
    "\n",
    "**Ответ:**\n",
    "\n",
    "1.Представление локального окна как квадратичной полиномиальной поверхности (двумерный полином второго порядка).   \n",
    "2. Оенка сдвига между двумя такими поверхностями путём решения линейной системы.   \n",
    "3. Итеративное уточнение: свернуть, сместить изображение, пересчитать.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вопрос 6\n",
    "\n",
    "Каким образом в методе Farneback обрабатываются большие смещения объектов между кадрами?\n",
    "\n",
    "**Ответ:**\n",
    "Используется многоуровневая (coarse-to-fine) пирамида: сначала поток оценивается на сильно сжатых кадрах, где даже большой реальный сдвиг выглядит маленьким, затем результат постепенно интерполируется и уточняется на более высоких разрешениях"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_optical_flow_farneback_opencv(video_path='data/slow_traffic_small.mp4', output_path='output_Farneback.mp4'):\n",
    "    \"\"\"\n",
    "    Демонстрация работы алгоритма плотного оптического потока Farneback на видео.\n",
    "\n",
    "    Args:\n",
    "        video_path: Путь к входному видео\n",
    "        output_path: Путь для сохранения результата\n",
    "    \"\"\"\n",
    "    # Открываем видео\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Получаем параметры видео\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Настраиваем запись выходного видео\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    # Берем первый кадр и преобразуем его в оттенки серого\n",
    "    ret, frame1 = cap.read()\n",
    "    if not ret:\n",
    "        print('Не удалось прочитать видео')\n",
    "        return None\n",
    "\n",
    "    prvs = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Создаем HSV-изображение для визуализации потока\n",
    "    hsv = np.zeros_like(frame1)\n",
    "    hsv[..., 1] = 255  # Насыщенность устанавливаем на максимум\n",
    "\n",
    "    from tqdm import tqdm\n",
    "    for i in tqdm(range(length - 1)):  # -1 потому что первый кадр мы уже прочитали\n",
    "        ret, frame2 = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            print('No frames grabbed!')\n",
    "            break\n",
    "\n",
    "        next_frame = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Вычисляем оптический поток методом Farneback\n",
    "        # Параметры:\n",
    "        # - 0.5: коэффициент масштабирования для пирамиды изображений\n",
    "        # - 3: кол-во уровней пирамиды\n",
    "        # - 15: размер окна для усреднения\n",
    "        # - 3: число итераций на каждом уровне пирамиды\n",
    "        # - 5: размер окна для полиномиальной аппроксимации\n",
    "        # - 1.2: стандартное отклонение для сглаживания\n",
    "        flow = cv2.calcOpticalFlowFarneback(\n",
    "            prvs, next_frame, None,\n",
    "            0.5, 3, 15, 3, 5, 1.2, 0\n",
    "        )\n",
    "\n",
    "        # Преобразуем векторы потока из декартовых координат в полярные\n",
    "        mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "\n",
    "        # Кодируем направление потока как оттенок (hue)\n",
    "        hsv[..., 0] = ang * 180 / np.pi / 2\n",
    "\n",
    "        # Кодируем величину потока как яркость (value)\n",
    "        hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "        # Преобразуем HSV в BGR для отображения\n",
    "        bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "        # Записываем результат\n",
    "        out.write(bgr)\n",
    "\n",
    "        # Обновляем предыдущий кадр\n",
    "        prvs = next_frame\n",
    "\n",
    "    # Освобождаем ресурсы\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "    print(f\"Результат сохранен в {output_path}\")\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x5634504d/'MP4V' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 913/913 [02:07<00:00,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результат сохранен в output_opencv_farneback.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result_path = demo_optical_flow_farneback_opencv(video_path='data/slow_traffic_small.mp4', output_path='output_opencv_farneback.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вопрос 7\n",
    "\n",
    "Как влияет предварительная обработка изображений (фильтрация шума, выравнивание гистограмм) на качество оптического потока, получаемого методом Farneback? Предложите оптимальный пайплайн предобработки.\n",
    "\n",
    "**Ответ:**\n",
    "1. Convert → grayscale (цветовая информация не используется).     \n",
    "2. Gaussian blur σ≈1–1.5 px — убираем высокочастотный шум.    \n",
    "3. CLAHE (clip ≈ 2.0, tile ≈ 8×8) — лёгкое локальное выравнивание контраста.    \n",
    "4. I′ = I / mean(I) или линейное выравнивание среднего/STD между парой кадров.      \n",
    "5. Global motion compensation (feature-based homography) для видео с дрожанием.    \n",
    "6. Farneback (пирамида = 5–6 уровней, winsize ≈ 9–15, iterations ≈ 3).     \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "torchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
